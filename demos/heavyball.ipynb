{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a344267-da1f-441a-9470-7ffa53801253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.linalg as tla\n",
    "import torch.autograd\n",
    "import numml.sparse as sp\n",
    "import numml.iterative as it\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d3b9ba-e2eb-4ed0-9115-298c266e22c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 12\n",
    "A = sp.eye(N)*2 - sp.eye(N,k=-1) - sp.eye(N,k=1)\n",
    "b = torch.ones(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d311b6-fe60-4786-9a7d-7e869e72b91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(A, b, x):\n",
    "    return (x@A@x)/2 - x@b\n",
    "\n",
    "def gradf(A, b, x):\n",
    "    return A@x - b\n",
    "\n",
    "def Hf(A, b, x):\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f9b20f-6e39-4592-a2c8-ddc4f93df38c",
   "metadata": {},
   "source": [
    "Consider the linear system of equations,\n",
    "$$ Ax = b, $$\n",
    "for $A \\in \\mathbb{R}^{n \\times n}$ and $x, b \\in \\mathbb{R}^n$.  Defining the quadratic loss function\n",
    "$$ \\begin{align} f\\left(A, b, x\\right) &= \\frac{1}{2}x^T A x - x^T b \\\\ \\nabla_f\\left(A, b, x\\right) &= Ax - b \\end{align}, $$\n",
    "we can write Polyak's heavy-ball iteration like\n",
    "$$ \\begin{align} x^{(k+1)} &= x^{(k)} - \\alpha \\nabla_f + \\beta\\left(x^{(k)} - x^{(k-1)}\\right) \\\\ &= x^{(k)} + \\alpha \\left(b-Ax^{(k)}\\right) + \\beta\\left(x^{(k)} - x^{(k-1)}\\right) \\end{align}. $$\n",
    "\n",
    "Since we have a nice adjoint solver for fixed-point problems, we will convert this to a FP problem by defining\n",
    "$$ \\bar{x}^{(k)} = \\begin{bmatrix} x^{(k)} \\\\ x^{(k-1)} \\end{bmatrix}, $$\n",
    "as well as the restriction operators $R_1, R_2$ like\n",
    "$$ \\begin{align} R_1\\bar{x}^{(k)} &= x^{(k)}, \\\\ R_2\\bar{x}^{(k)} &= x^{(k-1)}. \\end{align} $$\n",
    "This gives the fixed-point map\n",
    "$$ \\begin{align}\n",
    "g\\left(A, b, \\bar{x}\\right) &= R_1^T\\left(R_1\\bar{x} + \\alpha \\left(b - AR_1\\bar{x}\\right) + \\beta\\left(R_1\\bar{x} - R_2\\bar{x}\\right)\\right) + R_2^TR_1\\bar{x} \\\\\n",
    "&= \\begin{bmatrix}\n",
    "R_1\\bar{x} + \\alpha \\left(b - AR_1\\bar{x}\\right) + \\beta\\left(R_1\\bar{x} - R_2\\bar{x}\\right) \\\\ R_1\\bar{x}\n",
    "\\end{bmatrix}\n",
    "\\end{align},\n",
    "$$\n",
    "which at a fixed point will return $\\begin{bmatrix}x^\\star \\\\ x^\\star\\end{bmatrix}$, where $x^\\star = A^{-1}b$.  Showing that this is a contraction and that $x^\\star$ is an attracting fixed point is left as an exercise to the reader :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a681df07-9444-4f00-afd2-c182e8bb074f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_hb(x, A, b, alpha, beta):\n",
    "    # Heavyball iteration to solve Ax=b\n",
    "    \n",
    "    # Grab x^{(k)} and x^{(k-1)}\n",
    "    x_k = x[:N]\n",
    "    x_kp = x[N:]\n",
    "    \n",
    "    # Next iterate\n",
    "    x_kn = x_k - alpha * gradf(A, b, x_k) + beta * (x_k - x_kp)\n",
    "    \n",
    "    # Re-pack into \\bar{x}\n",
    "    return torch.cat((x_kn, x_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e935cf70-5304-483c-8e23-2790d38a0672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize heavyball parameters over loss of l = (||b - A @ x^{(k)}|| / ||b - A @ x^{(0)}||)\n",
    "# at each iteration, we generate N_b random right-hand-sides and average the loss over these.\n",
    "# The test loss is b=1.\n",
    "\n",
    "x = torch.cat((torch.ones(N), torch.ones(N)))\n",
    "alpha = torch.tensor(0.5, requires_grad=True)\n",
    "beta = torch.tensor(0.5, requires_grad=True)\n",
    "\n",
    "opt = torch.optim.Adam([alpha, beta], lr=0.01)\n",
    "lh = []\n",
    "ah = []\n",
    "bh = []\n",
    "N_b = 10\n",
    "\n",
    "def test_loss(alpha, beta):\n",
    "    with torch.no_grad():\n",
    "        xk, xkp = it.fp_wrapper(f_hb, x, A, b, alpha, beta, max_iter=N).reshape((2, -1))\n",
    "    return (tla.norm(b - A @ xk) / tla.norm(b - A@torch.ones(N))).item()\n",
    "\n",
    "print('| It | Train Loss | Test Loss |')\n",
    "for i in range(40):\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    loss = 0.\n",
    "    for j in range(N_b):\n",
    "        b_rand = torch.randn(N)\n",
    "        xk, xkp = it.fp_wrapper(f_hb, x, A, b_rand, alpha, beta, max_iter=N).reshape((2, -1))\n",
    "        loss += (tla.norm(b_rand - A @ xk) / tla.norm(b_rand - A@torch.ones(N))) / N_b\n",
    "    loss.backward()\n",
    "    \n",
    "    opt.step()\n",
    "    tl = test_loss(alpha, beta)\n",
    "    print(f'| {i:2} | {loss.item():10.3f} | {tl:9.3f} |')\n",
    "    \n",
    "    lh.append(tl)\n",
    "    ah.append(alpha.item())\n",
    "    bh.append(beta.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fc21b0-2cfd-4d07-b1fc-28f4fb40b0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(lh, 'k--')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Test Loss')\n",
    "\n",
    "ax2 = plt.gca().twinx()\n",
    "ax2.plot(ah, 'r', label='Alpha')\n",
    "ax2.plot(bh, 'b', label='Beta')\n",
    "ax2.set_ylabel('Heavy-ball Weight')\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4864a8c-4348-427a-bc41-585d32bdc82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution plots\n",
    "\n",
    "xg = it.fp_wrapper(f_hb, x, A, b, alpha, beta, max_iter=N)[:N]\n",
    "xg2 = it.fp_wrapper(f_hb, x, A, b, 0.5, 0.5, max_iter=N)[:N]\n",
    "plt.plot(sp.spsolve(A, b).detach(), 'k', label='True Soln')\n",
    "plt.plot(xg.detach(), 'r--', label=f'Opt. Heavy-ball Soln. (a={alpha:0.2f}, b={beta:0.2f})')\n",
    "plt.plot(xg2.detach(), 'b--', label=f'Naive Heavy-ball Soln. (a={0.5:0.2f}, b={0.5:0.2f})')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25e5801-0fd8-4982-bdf5-de271334f456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual plots\n",
    "\n",
    "def heavyball_res(x, A, b, alpha, beta):\n",
    "    N = A.shape[0]\n",
    "    res = torch.empty(N+1)\n",
    "    res[0] = tla.norm(b-A@x) / tla.norm(b)\n",
    "    \n",
    "    x_p = x\n",
    "    for i in range(1, N+1):\n",
    "        x_n = x - alpha * gradf(A, b, x) + beta * (x - x_p)\n",
    "        x_p = x\n",
    "        x = x_n\n",
    "        res[i] = tla.norm(b-A@x) / tla.norm(b)\n",
    "    \n",
    "    return res\n",
    "\n",
    "with torch.no_grad():\n",
    "    res_opt = heavyball_res(torch.ones(N), A, b, alpha, beta)\n",
    "    res_naive = heavyball_res(torch.ones(N), A, b, 0.5, 0.5)\n",
    "\n",
    "plt.semilogy(res_opt, 'r')\n",
    "plt.semilogy(res_naive, 'b')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Relative Residual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707384d6-086a-4668-90dc-6d5f39c65ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cf6d6e-552b-45df-b542-041b33e5212b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
