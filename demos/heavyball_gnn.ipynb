{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a344267-da1f-441a-9470-7ffa53801253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.linalg as tla\n",
    "import torch.autograd\n",
    "import torch.nn as tNN\n",
    "import numml.sparse as sp\n",
    "import numml.nn as nNN\n",
    "import numml.utils as utils\n",
    "import numml.krylov as kry\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ae0720-e8cb-4cc7-a2d8-c7dfe1297527",
   "metadata": {},
   "source": [
    "Consider the linear system of equations,\n",
    "$$ Ax = b, $$\n",
    "for $A \\in \\mathbb{R}^{n \\times n}$ and $x, b \\in \\mathbb{R}^n$.  Defining the quadratic loss function\n",
    "$$ \\begin{align} f\\left(A, b, x\\right) &= \\frac{1}{2}x^T A x - x^T b \\\\ \\nabla_f\\left(A, b, x\\right) &= Ax - b \\end{align}, $$\n",
    "we can write Polyak's heavy-ball iteration like\n",
    "$$ \\begin{align} x^{(k+1)} &= x^{(k)} - \\alpha \\nabla_f + \\beta\\left(x^{(k)} - x^{(k-1)}\\right) \\\\ &= x^{(k)} + \\alpha \\left(b-Ax^{(k)}\\right) + \\beta\\left(x^{(k)} - x^{(k-1)}\\right) \\end{align}. $$\n",
    "\n",
    "Since we have a nice adjoint solver for fixed-point problems, we will convert this to a FP problem by defining\n",
    "$$ \\bar{x}^{(k)} = \\begin{bmatrix} x^{(k)} \\\\ x^{(k-1)} \\end{bmatrix}, $$\n",
    "as well as the restriction operators $R_1, R_2$ like\n",
    "$$ \\begin{align} R_1\\bar{x}^{(k)} &= x^{(k)}, \\\\ R_2\\bar{x}^{(k)} &= x^{(k-1)}. \\end{align} $$\n",
    "This gives the fixed-point map\n",
    "$$ \\begin{align}\n",
    "g\\left(A, b, \\bar{x}\\right) &= R_1^T\\left(R_1\\bar{x} + \\alpha \\left(b - AR_1\\bar{x}\\right) + \\beta\\left(R_1\\bar{x} - R_2\\bar{x}\\right)\\right) + R_2^TR_1\\bar{x} \\\\\n",
    "&= \\begin{bmatrix}\n",
    "R_1\\bar{x} + \\alpha \\left(b - AR_1\\bar{x}\\right) + \\beta\\left(R_1\\bar{x} - R_2\\bar{x}\\right) \\\\ R_1\\bar{x}\n",
    "\\end{bmatrix}\n",
    "\\end{align},\n",
    "$$\n",
    "which at a fixed point will return $\\begin{bmatrix}x^\\star \\\\ x^\\star\\end{bmatrix}$, where $x^\\star = A^{-1}b$.  Showing that this is a contraction and that $x^\\star$ is an attracting fixed point is left as an exercise to the reader :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d3b9ba-e2eb-4ed0-9115-298c266e22c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem setup\n",
    "\n",
    "device = (torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "#device = torch.device('cpu')\n",
    "\n",
    "N = 24\n",
    "A = sp.eye(N)*2 - sp.eye(N,k=-1) - sp.eye(N,k=1)\n",
    "A = A.to(device)\n",
    "b = torch.zeros(N).to(device)\n",
    "bt = torch.ones(N).to(device)\n",
    "xt = kry.conjugate_gradient(A, bt)[0]\n",
    "\n",
    "max_it = 16\n",
    "\n",
    "network_H = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6759a623-c875-43af-9920-d021d6e59c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network definition.  We'll use a shallow network composed of two GCN layers.\n",
    "\n",
    "class Network(tNN.Module):\n",
    "    def __init__(self, H):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nNN.GCNConv(3, H, normalize=True)\n",
    "        self.conv2 = nNN.GCNConv(H, H, normalize=True)\n",
    "        self.conv3 = nNN.GCNConv(H, 2, normalize=True)\n",
    "    \n",
    "    def forward(self, A, xk, xkp, r):\n",
    "        X = torch.column_stack((xk, xkp, r))\n",
    "        X = torch.relu(self.conv1(A, X))\n",
    "        X = torch.relu(self.conv2(A, X))\n",
    "        X = torch.sigmoid(self.conv3(A, X))\n",
    "        return torch.mean(X, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d311b6-fe60-4786-9a7d-7e869e72b91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective functions\n",
    "\n",
    "def f(A, b, x):\n",
    "    return (x@A@x)/2 - x@b\n",
    "\n",
    "def gradf(A, b, x):\n",
    "    return A@x - b\n",
    "\n",
    "def Hf(A, b, x):\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a681df07-9444-4f00-afd2-c182e8bb074f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heavyball iteration\n",
    "\n",
    "def f_hb(x, A, b):   \n",
    "    # Grab x^{(k)} and x^{(k-1)}\n",
    "    x_k = x[:N]\n",
    "    x_kp = x[N:]\n",
    "    \n",
    "    alpha, beta = net(A, x_k, x_kp, b)\n",
    "    \n",
    "    # Next iterate\n",
    "    x_kn = x_k - alpha * gradf(A, b, x_k) + beta * (x_k - x_kp)\n",
    "    \n",
    "    # Re-pack into \\bar{x}\n",
    "    return torch.cat((x_kn, x_k))\n",
    "\n",
    "def f_hb_fixed(x, A, b, alphabeta):   \n",
    "    # Grab x^{(k)} and x^{(k-1)}\n",
    "    x_k = x[:N]\n",
    "    x_kp = x[N:]\n",
    "\n",
    "    alpha, beta = alphabeta\n",
    "    \n",
    "    # Next iterate\n",
    "    x_kn = x_k - alpha * gradf(A, b, x_k) + beta * (x_k - x_kp)\n",
    "    \n",
    "    # Re-pack into \\bar{x}\n",
    "    return torch.cat((x_kn, x_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaf9f6d-4f99-43f4-a99c-9e6c782263f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fp_wrapper(f, x, *args, max_iter=1):\n",
    "    for i in range(int(max_iter)):\n",
    "        x = f(x, *args)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00cc274-e1fc-4c61-906e-28b3412ac360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the network to output alpha and beta values at each iteration of heavyball\n",
    "\n",
    "xg = torch.randn(N, device=device)\n",
    "xg /= tla.norm(xg)\n",
    "x = torch.cat((xg, xg)).to(device)\n",
    "net = Network(network_H).to(device)\n",
    "\n",
    "opt = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "tr_lh = []\n",
    "te_lh = []\n",
    "N_b = 100\n",
    "\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9976d0b6-28b7-48f3-92ea-a30203f8d5d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('| It  | Train Loss | Test Loss |')\n",
    "\n",
    "def test_loss():\n",
    "    with torch.no_grad():\n",
    "        xk, xkp = fp_wrapper(f_hb, x, A, bt, max_iter=max_it).reshape((2, -1))\n",
    "    e = xt - xk\n",
    "    return (e@(A@e))\n",
    "\n",
    "Xt = torch.randn((N_b, N)).to(device)\n",
    "Xg = torch.randn((N_b, N)).to(device)\n",
    "B = (A@Xt.T).T\n",
    "\n",
    "while True:\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    loss = 0.\n",
    "    tr_loss = torch.zeros(N_b)\n",
    "    \n",
    "    for j in range(N_b):\n",
    "        x0 = torch.cat((Xg[j], Xg[j]))\n",
    "        xk, xkp = fp_wrapper(f_hb, x0, A, B[j], max_iter=max_it).reshape((2, -1))\n",
    "        \n",
    "        e = Xt[j] - xk\n",
    "        loss_it = (e@(A@e))\n",
    "        tr_loss[j] = loss_it\n",
    "        \n",
    "        loss += loss_it / N_b\n",
    "    loss.backward()\n",
    "        \n",
    "    opt.step()\n",
    "    tl = test_loss()\n",
    "    print(f'| {i:3} | {loss.item():10.5f} | {tl:9.3f} |')\n",
    "\n",
    "    tr_lh.append(tr_loss.detach().cpu().numpy())\n",
    "    te_lh.append(tl.detach().cpu().item())\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772c9509-9c60-4d95-81b0-7610ad10add6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.array(tr_lh))\n",
    "plt.ylabel('Training Loss')\n",
    "plt.xlabel('Iteration')\n",
    "plt.title('Ind. Training Loss')\n",
    "plt.grid()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(te_lh, 'r', label='Test loss')\n",
    "plt.ylabel('Testing loss')\n",
    "plt.xlabel('Iteration')\n",
    "plt.title('Test Loss')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4864a8c-4348-427a-bc41-585d32bdc82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution plots\n",
    "\n",
    "static_ab = (0.7, 0.7)\n",
    "\n",
    "xg = fp_wrapper(f_hb, x, A, bt, max_iter=N)[:N]\n",
    "xg2 = fp_wrapper(f_hb_fixed, x, A, bt, static_ab, max_iter=N)[:N]\n",
    "plt.plot(kry.conjugate_gradient(A, bt, x[:N])[0].detach().cpu().numpy(), 'k', label='True Soln')\n",
    "plt.plot(xg.detach().cpu().numpy(), 'r--', label=f'GNN Heavy-ball Soln.')\n",
    "plt.plot(xg2.detach().cpu().numpy(), 'b--', label=f'Heavy-ball Soln. (a={static_ab[0]:0.2f}, b={static_ab[1]:0.2f})')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25e5801-0fd8-4982-bdf5-de271334f456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual plots\n",
    "\n",
    "def heavyball_res(x, A, b, max_it=None, alphabeta=None):\n",
    "    N = A.shape[0]\n",
    "    res = torch.empty(max_it+1)\n",
    "    res[0] = tla.norm(b-A@x) / tla.norm(b)\n",
    "    \n",
    "    alphas = torch.empty(max_it)\n",
    "    betas = torch.empty(max_it)\n",
    "    \n",
    "    x_p = x\n",
    "    for i in range(1, max_it+1):\n",
    "        if alphabeta is None:\n",
    "            alpha, beta = net(A, x, x_p, bt)\n",
    "        else:\n",
    "            alpha, beta = alphabeta\n",
    "        \n",
    "        x_n = x - alpha * gradf(A, bt, x) + beta * (x - x_p)\n",
    "        x_p = x\n",
    "        x = x_n\n",
    "        res[i] = tla.norm(b-A@x) / tla.norm(b)\n",
    "        alphas[i-1] = alpha\n",
    "        betas[i-1] = beta\n",
    "    \n",
    "    return res, alphas, betas\n",
    "\n",
    "res_max_it = N*3\n",
    "\n",
    "with torch.no_grad():\n",
    "    res_opt, a_opt, b_opt = heavyball_res(torch.ones(N).to(device), A, bt, max_it=res_max_it)\n",
    "    res_naive, a_naive, b_naive = heavyball_res(torch.ones(N).to(device), A, bt, alphabeta=static_ab, max_it=res_max_it)\n",
    "\n",
    "plt.figure()\n",
    "plt.semilogy(res_opt, 'r', label='GNN')\n",
    "plt.semilogy(res_naive, 'b', label=f'a={static_ab[0]:.2f}, b={static_ab[1]:.2f}')\n",
    "plt.semilogy((torch.tensor(kry.conjugate_gradient(A, b, x[:N], iterations=N, rtol=1e-10)[1]).detach().cpu().numpy() / np.linalg.norm(b.cpu().numpy())), 'k', label='CG')\n",
    "#plt.plot([N, N]\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Relative Residual')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(torch.arange(1, res_max_it+1), a_opt, '.-', label='Opt A')\n",
    "plt.plot(torch.arange(1, res_max_it+1), b_opt, '.-', label='Opt B')\n",
    "plt.plot(torch.arange(1, res_max_it+1), a_naive, '.-', label='Static A')\n",
    "plt.plot(torch.arange(1, res_max_it+1), b_naive, '--', label='Static B')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Heavyball Parameters')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67afcfb3-1b4e-4f2f-9f60-a2196478a70d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
