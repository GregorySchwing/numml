{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a344267-da1f-441a-9470-7ffa53801253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.linalg as tla\n",
    "import torch.autograd\n",
    "import torch.nn as tNN\n",
    "import numml.sparse as sp\n",
    "import numml.iterative as it\n",
    "import numml.nn as nNN\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6759a623-c875-43af-9920-d021d6e59c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(tNN.Module):\n",
    "    def __init__(self, H):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nNN.GCNConv(3, H, normalize=False)\n",
    "        self.conv2 = nNN.GCNConv(H, 2, normalize=False)\n",
    "    \n",
    "    def forward(self, A, xk, xkp, b):\n",
    "        X = torch.column_stack((xk, xkp, b))\n",
    "        X = torch.tanh(self.conv1(A, X))\n",
    "        X = torch.sigmoid(self.conv2(A, X))\n",
    "        return torch.mean(X, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ae0720-e8cb-4cc7-a2d8-c7dfe1297527",
   "metadata": {},
   "source": [
    "Consider the linear system of equations,\n",
    "$$ Ax = b, $$\n",
    "for $A \\in \\mathbb{R}^{n \\times n}$ and $x, b \\in \\mathbb{R}^n$.  Defining the quadratic loss function\n",
    "$$ \\begin{align} f\\left(A, b, x\\right) &= \\frac{1}{2}x^T A x - x^T b \\\\ \\nabla_f\\left(A, b, x\\right) &= Ax - b \\end{align}, $$\n",
    "we can write Polyak's heavy-ball iteration like\n",
    "$$ \\begin{align} x^{(k+1)} &= x^{(k)} - \\alpha \\nabla_f + \\beta\\left(x^{(k)} - x^{(k-1)}\\right) \\\\ &= x^{(k)} + \\alpha \\left(b-Ax^{(k)}\\right) + \\beta\\left(x^{(k)} - x^{(k-1)}\\right) \\end{align}. $$\n",
    "\n",
    "Since we have a nice adjoint solver for fixed-point problems, we will convert this to a FP problem by defining\n",
    "$$ \\bar{x}^{(k)} = \\begin{bmatrix} x^{(k)} \\\\ x^{(k-1)} \\end{bmatrix}, $$\n",
    "as well as the restriction operators $R_1, R_2$ like\n",
    "$$ \\begin{align} R_1\\bar{x}^{(k)} &= x^{(k)}, \\\\ R_2\\bar{x}^{(k)} &= x^{(k-1)}. \\end{align} $$\n",
    "This gives the fixed-point map\n",
    "$$ \\begin{align}\n",
    "g\\left(A, b, \\bar{x}\\right) &= R_1^T\\left(R_1\\bar{x} + \\alpha \\left(b - AR_1\\bar{x}\\right) + \\beta\\left(R_1\\bar{x} - R_2\\bar{x}\\right)\\right) + R_2^TR_1\\bar{x} \\\\\n",
    "&= \\begin{bmatrix}\n",
    "R_1\\bar{x} + \\alpha \\left(b - AR_1\\bar{x}\\right) + \\beta\\left(R_1\\bar{x} - R_2\\bar{x}\\right) \\\\ R_1\\bar{x}\n",
    "\\end{bmatrix}\n",
    "\\end{align},\n",
    "$$\n",
    "which at a fixed point will return $\\begin{bmatrix}x^\\star \\\\ x^\\star\\end{bmatrix}$, where $x^\\star = A^{-1}b$.  Showing that this is a contraction and that $x^\\star$ is an attracting fixed point is left as an exercise to the reader :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d3b9ba-e2eb-4ed0-9115-298c266e22c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 12\n",
    "A = sp.eye(N)*2 - sp.eye(N,k=-1) - sp.eye(N,k=1)\n",
    "b = torch.ones(N)\n",
    "\n",
    "network_H = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d311b6-fe60-4786-9a7d-7e869e72b91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(A, b, x):\n",
    "    return (x@A@x)/2 - x@b\n",
    "\n",
    "def gradf(A, b, x):\n",
    "    return A@x - b\n",
    "\n",
    "def Hf(A, b, x):\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a681df07-9444-4f00-afd2-c182e8bb074f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_hb(x, A, b, net_param):\n",
    "    \n",
    "    # Grab x^{(k)} and x^{(k-1)}\n",
    "    x_k = x[:N]\n",
    "    x_kp = x[N:]\n",
    "    \n",
    "    # network parameters -> torch object\n",
    "    if isinstance(net_param, torch.Tensor):\n",
    "        net = Network(network_H)\n",
    "        nNN.vector_to_model(net, net_param)\n",
    "        alpha, beta = net(A, x_k, x_kp, (b-A@x_k))\n",
    "    else:\n",
    "        alpha, beta = net_param\n",
    "    \n",
    "    # Next iterate\n",
    "    x_kn = x_k - alpha * gradf(A, b, x_k) + beta * (x_k - x_kp)\n",
    "    \n",
    "    # Re-pack into \\bar{x}\n",
    "    return torch.cat((x_kn, x_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e935cf70-5304-483c-8e23-2790d38a0672",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.cat((torch.ones(N), torch.ones(N)))\n",
    "net = Network(network_H)\n",
    "net_param = nNN.model_to_vector(net).detach().clone()\n",
    "net_param.requires_grad = True\n",
    "\n",
    "opt = torch.optim.Adam([net_param], lr=0.025)\n",
    "tr_lh = []\n",
    "te_lh = []\n",
    "N_b = 10\n",
    "B = torch.randn((N_b, N))\n",
    "\n",
    "def test_loss(net_param):\n",
    "    with torch.no_grad():\n",
    "        xk, xkp = it.fp_wrapper(f_hb, x, A, b, net_param, max_iter=N).reshape((2, -1))\n",
    "    return (tla.norm(b - A @ xk) / tla.norm(b - A@torch.ones(N))).item()\n",
    "\n",
    "print('| It | Train Loss | Test Loss |')\n",
    "i = 0\n",
    "while True:\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    loss = 0.\n",
    "    for j in range(N_b):\n",
    "        b_rand = B[j]\n",
    "        xk, xkp = it.fp_wrapper(f_hb, x, A, b_rand, net_param, max_iter=N).reshape((2, -1))\n",
    "        loss += (tla.norm(b_rand - A @ xk) / tla.norm(b_rand - A@torch.ones(N))) / N_b\n",
    "    loss.backward()\n",
    "    \n",
    "    opt.step()\n",
    "    tl = test_loss(net_param)\n",
    "    print(f'| {i:2} | {loss.item():10.3f} | {tl:9.3f} |')\n",
    "    \n",
    "    if tl < 0.05:\n",
    "        break\n",
    "    \n",
    "    tr_lh.append(loss.item())\n",
    "    te_lh.append(tl)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772c9509-9c60-4d95-81b0-7610ad10add6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(tr_lh, 'k', label='Train Loss')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "ax1 = plt.gca()\n",
    "\n",
    "ax2 = plt.gca().twinx()\n",
    "ax2.plot(te_lh, 'r', label='Test loss')\n",
    "ax2.set_ylabel('Testing loss')\n",
    "\n",
    "plt.grid()\n",
    "ax1.legend()\n",
    "ax2.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4864a8c-4348-427a-bc41-585d32bdc82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution plots\n",
    "\n",
    "static_ab = (0.7, 0.7)\n",
    "\n",
    "xg = it.fp_wrapper(f_hb, x, A, b, net_param, max_iter=N)[:N]\n",
    "xg2 = it.fp_wrapper(f_hb, x, A, b, static_ab, max_iter=N)[:N]\n",
    "plt.plot(sp.spsolve(A, b).detach(), 'k', label='True Soln')\n",
    "plt.plot(xg.detach(), 'r--', label=f'GNN Heavy-ball Soln.')\n",
    "plt.plot(xg2.detach(), 'b--', label=f'Heavy-ball Soln. (a={static_ab[0]:0.2f}, b={static_ab[1]:0.2f})')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25e5801-0fd8-4982-bdf5-de271334f456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual plots\n",
    "\n",
    "def heavyball_res(x, A, b, net_param):\n",
    "    N = A.shape[0]\n",
    "    res = torch.empty(N+1)\n",
    "    res[0] = tla.norm(b-A@x) / tla.norm(b)\n",
    "    \n",
    "    alphas = torch.empty(N)\n",
    "    betas = torch.empty(N)\n",
    "    \n",
    "    if isinstance(net_param, torch.Tensor):\n",
    "        net = Network(network_H)\n",
    "        nNN.vector_to_model(net, net_param)\n",
    "    \n",
    "    x_p = x\n",
    "    for i in range(1, N+1):\n",
    "        if isinstance(net_param, torch.Tensor):\n",
    "            alpha, beta = net(A, x, x_p, b)\n",
    "        else:\n",
    "            alpha, beta = net_param\n",
    "        \n",
    "        x_n = x - alpha * gradf(A, b, x) + beta * (x - x_p)\n",
    "        x_p = x\n",
    "        x = x_n\n",
    "        res[i] = tla.norm(b-A@x) / tla.norm(b)\n",
    "        alphas[i-1] = alpha\n",
    "        betas[i-1] = beta\n",
    "    \n",
    "    return res, alphas, betas\n",
    "\n",
    "with torch.no_grad():\n",
    "    res_opt, a_opt, b_opt = heavyball_res(torch.ones(N), A, b, net_param)\n",
    "    res_naive, a_naive, b_naive = heavyball_res(torch.ones(N), A, b, static_ab)\n",
    "\n",
    "plt.figure()\n",
    "plt.semilogy(res_opt, 'r', label='GNN')\n",
    "plt.semilogy(res_naive, 'b', label=f'a={static_ab[0]:.2f}, b={static_ab[1]:.2f}')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Relative Residual')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(torch.arange(1, N+1), a_opt, '.-', label='Opt A')\n",
    "plt.plot(torch.arange(1, N+1), b_opt, '.-', label='Opt B')\n",
    "plt.plot(torch.arange(1, N+1), a_naive, '.-', label='Static A')\n",
    "plt.plot(torch.arange(1, N+1), b_naive, '--', label='Static B')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67afcfb3-1b4e-4f2f-9f60-a2196478a70d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
