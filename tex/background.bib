@misc{Johnson_notes,
author = {Justin Johnson},
title = {Derivatives, Backpropagation, and Vectorization},
month = {September},
year = {2017},
howpublished = "\url{http://cs231n.stanford.edu/handouts/derivatives.pdf}"
}

@article{pytorch,
  author    = {Adam Paszke and
               Sam Gross and
               Francisco Massa and
               Adam Lerer and
               James Bradbury and
               Gregory Chanan and
               Trevor Killeen and
               Zeming Lin and
               Natalia Gimelshein and
               Luca Antiga and
               Alban Desmaison and
               Andreas K{\"{o}}pf and
               Edward Z. Yang and
               Zach DeVito and
               Martin Raison and
               Alykhan Tejani and
               Sasank Chilamkurthy and
               Benoit Steiner and
               Lu Fang and
               Junjie Bai and
               Soumith Chintala},
  title     = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  journal   = {CoRR},
  volume    = {abs/1912.01703},
  year      = {2019},
  url       = {http://arxiv.org/abs/1912.01703},
  eprinttype = {arXiv},
  eprint    = {1912.01703},
  timestamp = {Tue, 02 Nov 2021 15:18:32 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1912-01703.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{BaydinPR15,
  author    = {Atilim Gunes Baydin and
               Barak A. Pearlmutter and
               Alexey Andreyevich Radul},
  title     = {Automatic differentiation in machine learning: a survey},
  journal   = {CoRR},
  volume    = {abs/1502.05767},
  year      = {2015},
  url       = {http://arxiv.org/abs/1502.05767},
  eprinttype = {arXiv},
  eprint    = {1502.05767},
  timestamp = {Mon, 13 Aug 2018 16:48:36 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/BaydinPR15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{tensors,
author = {Kolda, Tamara G. and Bader, Brett W.},
title = {Tensor Decompositions and Applications},
journal = {SIAM Review},
volume = {51},
number = {3},
pages = {455-500},
year = {2009},
doi = {10.1137/07070111X},
URL = {
        https://doi.org/10.1137/07070111X
},
eprint = {
        https://doi.org/10.1137/07070111X
}
,
    abstract = { This survey provides an overview of higher-order tensor decompositions, their applications, and available software. A tensor is a multidimensional or N-way array. Decompositions of higher-order tensors (i.e., N-way arrays with \$N \geq 3\$) have applications in psycho-metrics, chemometrics, signal processing, numerical linear algebra, computer vision, numerical analysis, data mining, neuroscience, graph analysis, and elsewhere. Two particular tensor decompositions can be considered to be higher-order extensions of the matrix singular value decomposition: CANDECOMP/PARAFAC (CP) decomposes a tensor as a sum of rank-one tensors, and the Tucker decomposition is a higher-order form of principal component analysis. There are many other tensor decompositions, including INDSCAL, PARAFAC2, CANDELINC, DEDICOM, and PARATUCK2 as well as nonnegative variants of all of the above. The N-way Toolbox, Tensor Toolbox, and Multilinear Engine are examples of software packages for working with tensors. }
}
